{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set environment parameters\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch. Ensure that the environment build is in the `python/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"./env/env\"  # Name of the Unity environment binary to launch\n",
    "train_mode = True  # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import sys\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 30\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 1\n",
      "        Vector Action descriptions: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 30\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 1\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Info\n",
    "\n",
    "\n",
    "Each dataset consists of the data as follows \n",
    "- position_x \n",
    "- position_y\n",
    "- velocity_x\n",
    "- velocity_y\n",
    "- (1/mass)\n",
    "- (1/scale)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get data from the environment to make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 / 100\n",
      "Data length: 1000\n",
      "Episode 1 / 100\n",
      "Data length: 2000\n",
      "Episode 2 / 100\n",
      "Data length: 3000\n",
      "Episode 3 / 100\n",
      "Data length: 4000\n",
      "Episode 4 / 100\n",
      "Data length: 5000\n",
      "Episode 5 / 100\n",
      "Data length: 6000\n",
      "Episode 6 / 100\n",
      "Data length: 7000\n",
      "Episode 7 / 100\n",
      "Data length: 8000\n",
      "Episode 8 / 100\n",
      "Data length: 9000\n",
      "Episode 9 / 100\n",
      "Data length: 10000\n",
      "Episode 10 / 100\n",
      "Data length: 11000\n",
      "Episode 11 / 100\n",
      "Data length: 12000\n",
      "Episode 12 / 100\n",
      "Data length: 13000\n",
      "Episode 13 / 100\n",
      "Data length: 14000\n",
      "Episode 14 / 100\n",
      "Data length: 15000\n",
      "Episode 15 / 100\n",
      "Data length: 16000\n",
      "Episode 16 / 100\n",
      "Data length: 17000\n",
      "Episode 17 / 100\n",
      "Data length: 18000\n",
      "Episode 18 / 100\n",
      "Data length: 19000\n",
      "Episode 19 / 100\n",
      "Data length: 20000\n",
      "Episode 20 / 100\n",
      "Data length: 21000\n",
      "Episode 21 / 100\n",
      "Data length: 22000\n",
      "Episode 22 / 100\n",
      "Data length: 23000\n",
      "Episode 23 / 100\n",
      "Data length: 24000\n",
      "Episode 24 / 100\n",
      "Data length: 25000\n",
      "Episode 25 / 100\n",
      "Data length: 26000\n",
      "Episode 26 / 100\n",
      "Data length: 27000\n",
      "Episode 27 / 100\n",
      "Data length: 28000\n",
      "Episode 28 / 100\n",
      "Data length: 29000\n",
      "Episode 29 / 100\n",
      "Data length: 30000\n",
      "Episode 30 / 100\n",
      "Data length: 31000\n",
      "Episode 31 / 100\n",
      "Data length: 32000\n",
      "Episode 32 / 100\n",
      "Data length: 33000\n",
      "Episode 33 / 100\n",
      "Data length: 34000\n",
      "Episode 34 / 100\n",
      "Data length: 35000\n",
      "Episode 35 / 100\n",
      "Data length: 36000\n",
      "Episode 36 / 100\n",
      "Data length: 37000\n",
      "Episode 37 / 100\n",
      "Data length: 38000\n",
      "Episode 38 / 100\n",
      "Data length: 39000\n",
      "Episode 39 / 100\n",
      "Data length: 40000\n",
      "Episode 40 / 100\n",
      "Data length: 41000\n",
      "Episode 41 / 100\n",
      "Data length: 42000\n",
      "Episode 42 / 100\n",
      "Data length: 43000\n",
      "Episode 43 / 100\n",
      "Data length: 44000\n",
      "Episode 44 / 100\n",
      "Data length: 45000\n",
      "Episode 45 / 100\n",
      "Data length: 46000\n",
      "Episode 46 / 100\n",
      "Data length: 47000\n",
      "Episode 47 / 100\n",
      "Data length: 48000\n",
      "Episode 48 / 100\n",
      "Data length: 49000\n",
      "Episode 49 / 100\n",
      "Data length: 50000\n",
      "Episode 50 / 100\n",
      "Data length: 51000\n",
      "Episode 51 / 100\n",
      "Data length: 52000\n",
      "Episode 52 / 100\n",
      "Data length: 53000\n",
      "Episode 53 / 100\n",
      "Data length: 54000\n",
      "Episode 54 / 100\n",
      "Data length: 55000\n",
      "Episode 55 / 100\n",
      "Data length: 56000\n",
      "Episode 56 / 100\n",
      "Data length: 57000\n",
      "Episode 57 / 100\n",
      "Data length: 58000\n",
      "Episode 58 / 100\n",
      "Data length: 59000\n",
      "Episode 59 / 100\n",
      "Data length: 60000\n",
      "Episode 60 / 100\n",
      "Data length: 61000\n",
      "Episode 61 / 100\n",
      "Data length: 62000\n",
      "Episode 62 / 100\n",
      "Data length: 63000\n",
      "Episode 63 / 100\n",
      "Data length: 64000\n",
      "Episode 64 / 100\n",
      "Data length: 65000\n",
      "Episode 65 / 100\n",
      "Data length: 66000\n",
      "Episode 66 / 100\n",
      "Data length: 67000\n",
      "Episode 67 / 100\n",
      "Data length: 68000\n",
      "Episode 68 / 100\n",
      "Data length: 69000\n",
      "Episode 69 / 100\n",
      "Data length: 70000\n",
      "Episode 70 / 100\n",
      "Data length: 71000\n",
      "Episode 71 / 100\n",
      "Data length: 72000\n",
      "Episode 72 / 100\n",
      "Data length: 73000\n",
      "Episode 73 / 100\n",
      "Data length: 74000\n",
      "Episode 74 / 100\n",
      "Data length: 75000\n",
      "Episode 75 / 100\n",
      "Data length: 76000\n",
      "Episode 76 / 100\n",
      "Data length: 77000\n",
      "Episode 77 / 100\n",
      "Data length: 78000\n",
      "Episode 78 / 100\n",
      "Data length: 79000\n",
      "Episode 79 / 100\n",
      "Data length: 80000\n",
      "Episode 80 / 100\n",
      "Data length: 81000\n",
      "Episode 81 / 100\n",
      "Data length: 82000\n",
      "Episode 82 / 100\n",
      "Data length: 83000\n",
      "Episode 83 / 100\n",
      "Data length: 84000\n",
      "Episode 84 / 100\n",
      "Data length: 85000\n",
      "Episode 85 / 100\n",
      "Data length: 86000\n",
      "Episode 86 / 100\n",
      "Data length: 87000\n",
      "Episode 87 / 100\n",
      "Data length: 88000\n",
      "Episode 88 / 100\n",
      "Data length: 89000\n",
      "Episode 89 / 100\n",
      "Data length: 90000\n",
      "Episode 90 / 100\n",
      "Data length: 91000\n",
      "Episode 91 / 100\n",
      "Data length: 92000\n",
      "Episode 92 / 100\n",
      "Data length: 93000\n",
      "Episode 93 / 100\n",
      "Data length: 94000\n",
      "Episode 94 / 100\n",
      "Data length: 95000\n",
      "Episode 95 / 100\n",
      "Data length: 96000\n",
      "Episode 96 / 100\n",
      "Data length: 97000\n",
      "Episode 97 / 100\n",
      "Data length: 98000\n",
      "Episode 98 / 100\n",
      "Data length: 99000\n",
      "Episode 99 / 100\n",
      "Data length: 100000\n",
      "(100000, 6, 6)\n",
      "(100000, 6, 6)\n",
      "Data is saved!!!\n"
     ]
    }
   ],
   "source": [
    "def get_state(env):\n",
    "    state = np.zeros([6, 6])\n",
    "    vector_obs = env.vector_observations[0]\n",
    "    vector_obs_reshape = np.reshape(vector_obs, (5, 6))\n",
    "    vector_obs_trans = np.transpose(vector_obs_reshape)\n",
    "    state[:, :-1] = vector_obs_trans\n",
    "    # 1/Mass and 1/scale of wall is 0\n",
    "    state[4, -1] = 0\n",
    "    state[5, -1] = 0\n",
    "    \n",
    "    return state\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "num_episode = 100\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    state = get_state(env_info)\n",
    "    \n",
    "    while True:\n",
    "        # Random action \n",
    "        env_info = env.step(np.random.randint(0, brain.vector_action_space_size, \n",
    "                                                  size=(len(env_info.agents))))[default_brain]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        next_state = get_state(env_info)\n",
    "        \n",
    "        data_x.append(state)\n",
    "        data_y.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    \n",
    "    print(\"Episode \" + str(episode) + ' / ' + str(num_episode) )\n",
    "    print(\"Data length: \" + str(len(data_x)))\n",
    "    \n",
    "X = np.asarray(data_x)\n",
    "Y = np.asarray(data_y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "data_dict = {'X': X, 'Y': Y}\n",
    "\n",
    "scipy.io.savemat('./dataset.mat', data_dict)\n",
    "\n",
    "print('Data is saved!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train1 = scipy.io.loadmat('./dataset_train1.mat')\n",
    "# dataset_train2 = scipy.io.loadmat('./dataset_train2.mat')\n",
    "# dataset_train3 = scipy.io.loadmat('./dataset_train3.mat')\n",
    "# dataset_train4 = scipy.io.loadmat('./dataset_train4.mat')\n",
    "\n",
    "# dataset_test = scipy.io.loadmat('./dataset_test.mat')\n",
    "\n",
    "# data_1x = dataset_train1['X']\n",
    "# data_1y = dataset_train1['Y']\n",
    "\n",
    "# data_2x = dataset_train2['X']\n",
    "# data_2y = dataset_train2['Y']\n",
    "\n",
    "# data_3x = dataset_train3['X']\n",
    "# data_3y = dataset_train3['Y']\n",
    "\n",
    "# data_4x = dataset_train4['X']\n",
    "# data_4y = dataset_train4['Y']\n",
    "\n",
    "# data_x = np.concatenate([data_1x, data_2x, data_3x, data_4x], axis = 0)\n",
    "# data_y = np.concatenate([data_1y, data_2y, data_3y, data_4y], axis = 0)\n",
    "\n",
    "# data_x_test = dataset_test['X']\n",
    "# data_y_test = dataset_test['Y']\n",
    "\n",
    "# normalization_factor = [1.0/5, 1.0/5, 1.0/20, 1.0/20, 1.0, 1.0]\n",
    "\n",
    "# for i in range(len(normalization_factor)):\n",
    "#     data_x[:,i,:] = data_x[:,i,:] * normalization_factor[i]\n",
    "#     data_y[:,i,:] = data_y[:,i,:] * normalization_factor[i]\n",
    "    \n",
    "#     data_x_test[:,i,:] = data_x_test[:,i,:] * normalization_factor[i]\n",
    "#     data_y_test[:,i,:] = data_y_test[:,i,:] * normalization_factor[i]\n",
    "    \n",
    "# Train_data_dict = {'X': data_x, 'Y': data_y}\n",
    "# Test_data_dict = {'X': data_x_test, 'Y': data_y_test}\n",
    "\n",
    "# scipy.io.savemat('./Training_dataset.mat', Train_data_dict)\n",
    "# scipy.io.savemat('./Testing_dataset.mat', Test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
