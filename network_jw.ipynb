{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "# Initialize weights and bias \n",
    "def weight_variable(name, shape_in):\n",
    "    return tf.get_variable(name,shape=shape_in, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(name, shape_in):\n",
    "    return tf.get_variable(name,shape=shape_in, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 6, 6) (500000, 6, 6)\n",
      "[0.66666667 1.         1.         1.24999998 0.8333333  0.        ]\n",
      "[0.66666667 1.         1.         1.24999998 0.8333333  0.        ]\n"
     ]
    }
   ],
   "source": [
    "data1 = scipy.io.loadmat('data/dataset.mat')\n",
    "data2 = scipy.io.loadmat('data/dataset2.mat')\n",
    "\n",
    "data_x = np.concatenate([data1['X'], data2['X']], axis = 0)\n",
    "data_y = np.concatenate([data1['Y'], data2['Y']], axis = 0)\n",
    "\n",
    "normalization_factor = [1.0/5, 1.0/5, 1.0/20, 1.0/20, 1.0, 1.0]\n",
    "normalization_bias = [0.0, 0, 0, 0, 0, 0]\n",
    "\n",
    "data_x[:,4,:] = 1/data_x[:,4,:]\n",
    "data_y[:,4,:] = 1/data_y[:,4,:]\n",
    "data_x[:,5,:] = 1/data_x[:,5,:]\n",
    "data_y[:,5,:] = 1/data_y[:,5,:]\n",
    "data_x[:,4,5] = 0\n",
    "data_y[:,4,5] = 0\n",
    "data_x[:,5,5] = 0\n",
    "data_y[:,5,5] = 0\n",
    "\n",
    "for i in range(len(normalization_factor)):\n",
    "    data_x[:,i,:] = data_x[:,i,:] * normalization_factor[i] + normalization_bias[i]\n",
    "    data_y[:,i,:] = data_y[:,i,:] * normalization_factor[i] + normalization_bias[i]\n",
    "\n",
    "print(np.shape(data_x), np.shape(data_y))\n",
    "\n",
    "print(data_x[0,5,:])\n",
    "print(data_y[0,5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interaction_net(O,Rr,Rs,Ra,X, output_len):\n",
    "\n",
    "    object_state = O.get_shape()[-2]\n",
    "    object_num = O.get_shape()[-1]\n",
    "    \n",
    "    relation_num = Rr.get_shape()[-1]\n",
    "    \n",
    "    B = tf.concat([tf.matmul(O,Rr),tf.matmul(O,Rs),Ra], axis = 1)\n",
    "    \n",
    "    B_len = B.get_shape()[-2]\n",
    "\n",
    "    print(O.get_shape())\n",
    "    print(Rr.get_shape())\n",
    "    print(B.get_shape())\n",
    "    \n",
    "    fR_hidden_num = 100\n",
    "    fR_hidden_layer = 3\n",
    "    \n",
    "    fR_weight = [weight_variable('fR_w_h'+str(i), [fR_hidden_num, fR_hidden_num]) for i in range(fR_hidden_layer-1)]\n",
    "    fR_weight.insert(0, weight_variable('fR_w_input', [B_len, fR_hidden_num]))\n",
    "    fR_bias = [bias_variable('fR_b_h'+str(i), [fR_hidden_num]) for i in range(fR_hidden_layer-1)]\n",
    "    fR_bias.insert(0, bias_variable('fR_b_input',[fR_hidden_num]))\n",
    "    \n",
    "    e_list = []\n",
    "    \n",
    "    for i in range(relation_num):\n",
    "        temp_B = tf.reshape(tf.slice(B,[0,0,i],[-1,-1,1]),[-1, B_len])\n",
    "        for layer in range(fR_hidden_layer):\n",
    "            if layer == 0:\n",
    "                fR_hidden_state = tf.nn.relu(tf.matmul(temp_B, fR_weight[layer]) + fR_bias[layer])\n",
    "            else:\n",
    "                fR_hidden_state = tf.nn.relu(tf.matmul(fR_hidden_state, fR_weight[layer]) + fR_bias[layer])\n",
    "        \n",
    "        e_list.append(fR_hidden_state)\n",
    "        \n",
    "    E = tf.stack(e_list, axis = 2)\n",
    "    E_bar = tf.matmul(E, Rr, transpose_b = True)\n",
    "    \n",
    "    print(E.get_shape())\n",
    "    print(E_bar.get_shape())\n",
    "    \n",
    "    C = tf.concat([O, X, E_bar], axis = 1)\n",
    "    \n",
    "    C_len = C.get_shape()[-2]\n",
    "    \n",
    "    print(C.get_shape())\n",
    "    \n",
    "    fO_hidden_num = 150\n",
    "    fO_hidden_layer = 3\n",
    "    \n",
    "    fO_weight = [weight_variable('fO_w_h'+str(i), [fO_hidden_num, fO_hidden_num]) for i in range(fO_hidden_layer-1)]\n",
    "    fO_weight.insert(0, weight_variable('fO_w_input', [C_len, fO_hidden_num]))\n",
    "    fO_bias = [bias_variable('fO_b_h'+str(i), [fO_hidden_num]) for i in range(fO_hidden_layer-1)]\n",
    "    fO_bias.insert(0, bias_variable('fO_b_input',[fO_hidden_num]))\n",
    "    \n",
    "    output_weight = weight_variable('out_w', [fO_hidden_num, output_len])\n",
    "    output_bias = weight_variable('out_b', [output_len])\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for i in range(object_num):\n",
    "        temp_C = tf.reshape(tf.slice(C,[0,0,i],[-1,-1,1]),[-1, C_len])\n",
    "        for layer in range(fO_hidden_layer):\n",
    "            if layer == 0:\n",
    "                fO_hidden_state = tf.nn.relu(tf.matmul(temp_C, fO_weight[layer]) + fO_bias[layer])\n",
    "            else:\n",
    "                fO_hidden_state = tf.nn.relu(tf.matmul(fO_hidden_state, fO_weight[layer]) + fO_bias[layer])\n",
    "                \n",
    "        temp_out = tf.matmul(fO_hidden_state, output_weight) + output_bias\n",
    "        out_list.append(temp_out)\n",
    "    \n",
    "    output = tf.stack(out_list, axis = 2)\n",
    "    \n",
    "    print(output)        \n",
    "    \n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6, 6)\n",
      "(?, 6, 30)\n",
      "(?, 13, 30)\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "(?, 100, 30)\n",
      "(?, 100, 6)\n",
      "(?, 107, 6)\n",
      "Tensor(\"stack_1:0\", shape=(?, 4, 6), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "object_state = np.shape(data_x)[-2]\n",
    "object_num = np.shape(data_x)[-1]\n",
    "# object_num = 1\n",
    "\n",
    "relation_num = 30\n",
    "relation_state = 1\n",
    "\n",
    "external_state = 1\n",
    "\n",
    "object_input = tf.placeholder(tf.float32, [None, object_state, object_num])\n",
    "relation_r = tf.placeholder(tf.float32, [None, object_num, relation_num])\n",
    "relation_s = tf.placeholder(tf.float32, [None, object_num, relation_num])\n",
    "relation_a = tf.placeholder(tf.float32, [None, relation_state, relation_num])\n",
    "external = tf.placeholder(tf.float32, [None, external_state, object_num])\n",
    "\n",
    "loss_state_num = 4\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, loss_state_num, object_num])\n",
    "\n",
    "predictions = interaction_net(object_input,relation_r,relation_s,relation_a,external, output_len = loss_state_num)\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(tf.square(y - predictions), axis = 1), axis = 1)) + 1\n",
    "\n",
    "print(loss.get_shape())\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, [============================= ] loss = 1.00088\n",
      "epoch 1/10, loss= 1.0019\n",
      "epoch 2/10, [============================= ] loss = 1.00062\n",
      "epoch 2/10, loss= 1.0016\n",
      "epoch 3/10, [============================= ] loss = 1.00048\n",
      "epoch 3/10, loss= 1.0013\n",
      "epoch 4/10, [============================= ] loss = 1.00039\n",
      "epoch 4/10, loss= 1.00113\n",
      "epoch 5/10, [============================= ] loss = 1.00014\n",
      "epoch 5/10, loss= 1.00109\n",
      "epoch 6/10, [============================= ] loss = 1.00334\n",
      "epoch 6/10, loss= 1.00107\n",
      "epoch 7/10, [============================= ] loss = 1.00008\n",
      "epoch 7/10, loss= 1.00103\n",
      "epoch 8/10, [============================= ] loss = 1.00013\n",
      "epoch 8/10, loss= 1.00105\n",
      "epoch 9/10, [============================= ] loss = 1.00418\n",
      "epoch 9/10, loss= 1.00098\n",
      "epoch 10/10, [============================= ] loss = 1.00084\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 256\n",
    "epoch_num = 10\n",
    "\n",
    "train_data_num = np.shape(data_x)[0]\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    order = np.arange(train_data_num)\n",
    "    np.random.shuffle(order)\n",
    "    temp_train_x = data_x[order]\n",
    "    temp_train_y = data_y[order]\n",
    "    for batch_step in range(int(train_data_num/batch_size)+1):\n",
    "        batch_idx = [batch_step*batch_size, (batch_step+1)*batch_size]\n",
    "        if batch_idx[1] > train_data_num: batch_idx[1] = train_data_num\n",
    "        \n",
    "        batch_x = temp_train_x[batch_idx[0]:batch_idx[1]]\n",
    "        batch_y = temp_train_y[batch_idx[0]:batch_idx[1]]\n",
    "        batch_y = batch_y[:,0:loss_state_num,:]\n",
    "        \n",
    "        batch_Rr = np.zeros([batch_idx[1]-batch_idx[0], object_num, relation_num])\n",
    "        batch_Rs = np.zeros([batch_idx[1]-batch_idx[0], object_num, relation_num])\n",
    "        batch_Ra = np.zeros([batch_idx[1]-batch_idx[0], relation_state, relation_num])\n",
    "        \n",
    "        batch_external = np.zeros([batch_idx[1]-batch_idx[0], external_state, object_num])\n",
    "        \n",
    "        relation_idx = 0\n",
    "        for i in range(object_num):\n",
    "            for j in range(object_num):\n",
    "                if i is not j:\n",
    "                    batch_Rs[:,i,relation_idx] = 1\n",
    "                    batch_Rr[:,j,relation_idx] = 1\n",
    "                    relation_idx = relation_idx + 1\n",
    "                    \n",
    "        train_step.run(feed_dict={object_input: batch_x, y: batch_y, relation_r: batch_Rr, relation_s: batch_Rs, relation_a: batch_Ra, external: batch_external, lr: 0.0001})\n",
    "        loss_value = loss.eval(feed_dict={object_input: batch_x, y: batch_y, relation_r: batch_Rr, relation_s: batch_Rs, relation_a: batch_Ra, external: batch_external})\n",
    "        print_num = int((batch_step/(int(train_data_num/batch_size)+1))*30)\n",
    "        print_string = \"epoch %d/%d, [\"%(epoch+1,epoch_num)+\"=\"*print_num+\" \"*(30-print_num) +\"] loss = \"+\"%g\"%(loss_value)\n",
    "        print(print_string, end=\"\\r\")\n",
    "        \n",
    "    \n",
    "    sample = 2048\n",
    "    temp_loss=np.zeros([int(train_data_num/sample)+1])\n",
    "    for batch_step in range(int(train_data_num/sample)+1):\n",
    "        batch_idx = [batch_step*sample, (batch_step+1)*sample]\n",
    "        if batch_idx[1] > train_data_num: batch_idx[1] = train_data_num\n",
    "            \n",
    "        batch_x = temp_train_x[batch_idx[0]:batch_idx[1]]\n",
    "        batch_y = temp_train_y[batch_idx[0]:batch_idx[1]]\n",
    "        batch_y = batch_y[:,0:loss_state_num,:]\n",
    "        \n",
    "        batch_Rr = np.zeros([batch_idx[1]-batch_idx[0], object_num, relation_num])\n",
    "        batch_Rs = np.zeros([batch_idx[1]-batch_idx[0], object_num, relation_num])\n",
    "        batch_Ra = np.zeros([batch_idx[1]-batch_idx[0], relation_state, relation_num])\n",
    "        \n",
    "        batch_external = np.zeros([batch_idx[1]-batch_idx[0], external_state, object_num])\n",
    "        \n",
    "        relation_idx = 0\n",
    "        for i in range(object_num):\n",
    "            for j in range(object_num):\n",
    "                if i is not j:\n",
    "                    batch_Rs[:,i,relation_idx] = 1\n",
    "                    batch_Rr[:,j,relation_idx] = 1\n",
    "                    relation_idx = relation_idx + 1\n",
    "        loss_value = loss.eval(feed_dict={object_input: batch_x, y: batch_y, relation_r: batch_Rr, relation_s: batch_Rs, relation_a: batch_Ra, external: batch_external})\n",
    "        temp_loss[batch_step] = loss_value * (batch_idx[1]-batch_idx[0])\n",
    "        # print(\"temp accuracy %g\"%temp_temp)\n",
    "    temp_loss = np.sum(temp_loss)/train_data_num\n",
    "    \n",
    "    print(\"\\nepoch %d/%d, loss= %g\"%(epoch+1, epoch_num, temp_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.trainable_variables(scope=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "test_idx = 50\n",
    "\n",
    "batch_x = data_x[test_idx:(test_idx+1)]\n",
    "batch_y = data_y[test_idx:(test_idx+1)]\n",
    "batch_y = batch_y[:,0:loss_state_num,:]\n",
    "\n",
    "batch_Rr = np.zeros([1, object_num, relation_num])\n",
    "batch_Rs = np.zeros([1, object_num, relation_num])\n",
    "batch_Ra = np.zeros([1, relation_state, relation_num])\n",
    "\n",
    "batch_external = np.zeros([1, external_state, object_num])\n",
    "\n",
    "relation_idx = 0\n",
    "for i in range(object_num):\n",
    "    for j in range(object_num):\n",
    "        if i is not j:\n",
    "            batch_Rs[:,i,relation_idx] = 1\n",
    "            batch_Rr[:,j,relation_idx] = 1\n",
    "            relation_idx = relation_idx + 1\n",
    "\n",
    "pred = predictions.eval(feed_dict={object_input: batch_x, y: batch_y, relation_r: batch_Rr, relation_s: batch_Rs, relation_a: batch_Ra, external: batch_external})\n",
    "# print(pred)\n",
    "plt.plot(data_x[test_idx,0,:], data_x[test_idx,1,:], 'go')\n",
    "plt.plot(data_y[test_idx,0,:], data_y[test_idx,1,:], 'ro')\n",
    "plt.plot(pred[0,0,:], pred[0,1,:], 'gx')\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
